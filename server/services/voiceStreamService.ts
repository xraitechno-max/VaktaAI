import type { VoiceWebSocketClient, VoiceMessage, TTSChunkMessage, PhonemeTTSChunkMessage, TranscriptionMessage, TTSStartMessage, TTSEndMessage } from '../types/voiceWebSocket';
import { sarvamVoiceService } from './sarvamVoice';
import { AssemblyAI } from 'assemblyai';
import { ObjectStorageService } from '../objectStorage';
import { storage } from '../storage';
import { tutorSessionService } from './tutorSessionService';
import { intentClassifier } from './intentClassifier';
import { emotionDetector } from './emotionDetector';
import { LanguageDetectionEngine, type DetectedLanguage } from './LanguageDetectionEngine';
import { SessionContextManager } from './SessionContextManager';
import { DynamicPromptEngine } from './DynamicPromptEngine';
import { ResponseValidator } from './ResponseValidator';
import { optimizedAI } from './optimizedAIService';
import { enhancedVoiceService } from './enhancedVoiceService';
import { performanceOptimizer, metricsTracker } from './PerformanceOptimizer';
import { hintService } from './hintService';
import { ttsCacheService } from './ttsCacheService';
import { audioCompression } from './audioCompression';
import { ttsMetrics } from './ttsMetrics';
import { voiceService } from './voiceService';
import { mapPollyVisemesToUnityPhonemes } from '../utils/visemeMapping';
import { avatarStateService } from './avatarStateService';
import { TTSTextProcessor } from '../utils/tts-text-processor';
import { ttsRouter } from './tts';

// Initialize AI Tutor services
const languageDetector = new LanguageDetectionEngine();
const sessionContextManager = new SessionContextManager();
const dynamicPromptEngine = new DynamicPromptEngine();
const responseValidator = new ResponseValidator();

const assemblyAI = new AssemblyAI({
  apiKey: process.env.ASSEMBLYAI_API_KEY || '',
});

export class VoiceStreamService {
  private objectStorage: ObjectStorageService;

  constructor() {
    this.objectStorage = new ObjectStorageService();
  }

  /**
   * Process audio chunks from client and transcribe in real-time
   * Supports streaming STT with Sarvam (primary) and AssemblyAI (fallback)
   */
  async processAudioChunk(
    ws: VoiceWebSocketClient,
    audioData: string, // Base64 encoded audio
    format: 'webm' | 'opus' | 'wav',
    isLast: boolean,
    language: 'hi' | 'en' = 'en'
  ): Promise<void> {
    try {
      // Decode base64 audio
      const audioBuffer = Buffer.from(audioData, 'base64');

      // Add to buffer
      if (!ws.audioBuffer) {
        ws.audioBuffer = [];
      }
      ws.audioBuffer.push(audioBuffer);

      console.log(`[VOICE STREAM] Received audio chunk: ${audioBuffer.length} bytes (isLast: ${isLast})`);

      // If this is the last chunk, process the complete audio
      if (isLast && ws.audioBuffer.length > 0) {
        const completeAudio = Buffer.concat(ws.audioBuffer);
        ws.audioBuffer = []; // Clear buffer

        console.log(`[VOICE STREAM] Processing complete audio: ${completeAudio.length} bytes`);

        // Send interim "processing" message to show STT is in progress
        const interimMsg: TranscriptionMessage = {
          type: 'TRANSCRIPTION',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          text: '...',  // Indicate processing
          confidence: 0,
          language: language,
          isFinal: false
        };
        ws.send(JSON.stringify(interimMsg));

        // Transcribe using Sarvam (primary) or AssemblyAI (fallback)
        const transcription = await this.transcribeAudio(completeAudio, language);

        // Send final transcription result to client
        const transcriptionMsg: TranscriptionMessage = {
          type: 'TRANSCRIPTION',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          text: transcription.text,
          confidence: transcription.confidence,
          language: transcription.language as 'hi' | 'en',
          isFinal: true
        };

        ws.send(JSON.stringify(transcriptionMsg));

        console.log(`[VOICE STREAM] ‚úÖ Transcription sent: "${transcription.text}"`);

        // üî• AUTO-TRIGGER AI TUTOR PIPELINE after successful transcription
        if (transcription.text && transcription.text.trim().length > 0 && ws.chatId && ws.userId) {
          console.log(`[VOICE STREAM] ‚Üí Triggering AI Tutor pipeline for transcription`);
          await this.processTutorResponse(
            ws,
            transcription.text,
            ws.chatId,
            ws.userId,
            transcription.language as 'hi' | 'en'
          );
        }
      }
    } catch (error) {
      console.error('[VOICE STREAM] Audio processing error:', error);
      
      const errorMsg: VoiceMessage = {
        type: 'ERROR',
        timestamp: new Date().toISOString(),
        code: 'AUDIO_PROCESSING_ERROR',
        message: 'Failed to process audio',
        recoverable: true
      };
      ws.send(JSON.stringify(errorMsg));
    }
  }

  /**
   * Transcribe audio using Sarvam AI (primary) or AssemblyAI (fallback)
   */
  private async transcribeAudio(
    audioBuffer: Buffer,
    language: 'hi' | 'en'
  ): Promise<{ text: string; confidence: number; language: string }> {
    // Try Sarvam AI first (Indian accent optimized)
    if (sarvamVoiceService.isAvailable()) {
      try {
        console.log('[VOICE STREAM] Using Sarvam AI for STT...');
        return await sarvamVoiceService.transcribeAudio(audioBuffer, language);
      } catch (error) {
        console.warn('[VOICE STREAM] Sarvam STT failed, falling back to AssemblyAI:', error);
      }
    }

    // Fallback to AssemblyAI
    try {
      console.log('[VOICE STREAM] Using AssemblyAI for STT...');
      
      // AssemblyAI supports direct file upload via their upload API
      // Upload the audio buffer directly (works with any format: WAV, WebM, Opus, etc.)
      const uploadUrl = await assemblyAI.files.upload(audioBuffer);
      
      const transcript = await assemblyAI.transcripts.transcribe({
        audio: uploadUrl,
        language_code: language === 'hi' ? 'hi' : 'en',
      });

      if (transcript.status === 'error') {
        throw new Error(`Transcription failed: ${transcript.error}`);
      }

      return {
        text: transcript.text || '',
        confidence: transcript.confidence || 0,
        language
      };
    } catch (error) {
      console.error('[VOICE STREAM] AssemblyAI STT error:', error);
      throw new Error('All STT providers failed');
    }
  }

  /**
   * üöÄ PHASE 1: Stream TTS chunks with REAL-TIME sentence-by-sentence generation
   * Detects sentence boundaries and generates TTS in parallel for <1.5s latency
   */
  async streamTTSChunksRealtime(
    ws: VoiceWebSocketClient,
    text: string,
    language: 'hi' | 'en',
    emotion?: string,
    intent?: string,
    personaId?: string
  ): Promise<void> {
    try {
      console.log(`[STREAMING TTS] üöÄ Real-time sentence-by-sentence TTS starting...`);
      
      // Sentence boundary regex (Hindi + English)
      const sentenceBoundary = /[‡•§.!?]\s+|[‡•§.!?]$/;
      
      // Split text into sentences
      const parts = text.split(sentenceBoundary);
      const sentences: string[] = [];
      for (let i = 0; i < parts.length; i++) {
        const part = parts[i].trim();
        if (part) {
          sentences.push(part);
        }
      }
      
      console.log(`[STREAMING TTS] Split into ${sentences.length} sentences`);
      
      // üî• FIX #2: Send TTS_START to reset client queue state
      const startMsg: TTSStartMessage = {
        type: 'TTS_START',
        timestamp: new Date().toISOString(),
        sessionId: ws.sessionId,
        text: text.substring(0, 100)
      };
      ws.send(JSON.stringify(startMsg));
      ws.isTTSActive = true;
      
      // Voice options for all chunks
      const voiceOptions = {
        emotion,
        intent,
        personaId,
        language,
        enableMathSpeech: true,
        enablePauses: true,
        enableEmphasis: true
      };
      
      // üî• Generate TTS for all sentences IN PARALLEL (don't await!)
      const ttsPromises = sentences.map(async (sentence, index) => {
        // üî• FIX #3: Assign deterministic sequence numbers BEFORE synthesis
        const sequenceNumber = index;
        
        try {
          const startTime = Date.now();
          
          // üöÄ PHASE 2.1: Check cache first
          const cachedAudio = await ttsCacheService.get(sentence, language, emotion, personaId);
          
          let audioBuffer: Buffer;
          let cached = false;
          
          if (cachedAudio) {
            audioBuffer = cachedAudio;
            cached = true;
          } else {
            // Generate TTS audio
            audioBuffer = await enhancedVoiceService.synthesize(sentence, voiceOptions);
            
            // üöÄ PHASE 2.1: Store in cache for future use
            await ttsCacheService.set(sentence, language, audioBuffer, emotion, personaId);
          }
          
          const genTime = Date.now() - startTime;
          
          const cacheStatus = cached ? 'üíæ CACHED' : 'üî® GENERATED';
          console.log(`[STREAMING TTS] ‚úÖ Chunk ${index + 1}/${sentences.length} ${cacheStatus} (${genTime}ms): "${sentence.substring(0, 40)}..."`);
          
          // üöÄ PHASE 2.2: Compress audio before sending (if beneficial)
          let finalAudioData: string;
          let compressed = false;
          let compressedSize = 0;
          
          if (audioCompression.shouldCompress(audioBuffer.length)) {
            const compressionResult = await audioCompression.compress(audioBuffer);
            finalAudioData = compressionResult.compressed.toString('base64');
            compressed = true;
            compressedSize = compressionResult.compressedSize;
          } else {
            finalAudioData = audioBuffer.toString('base64');
          }
          
          // üöÄ PHASE 2.4: Record metrics
          ttsMetrics.record({
            sentence,
            language,
            generationTime: genTime,
            cached,
            compressed,
            audioSize: audioBuffer.length,
            compressedSize: compressed ? compressedSize : undefined,
            sequence: sequenceNumber,
            sessionId: ws.sessionId,
          });
          
          // ‚úÖ CORRECT: Send TTS chunk with flat payload (matches TTSChunkMessage)
          const ttsMsg: TTSChunkMessage = {
            type: 'TTS_CHUNK',
            timestamp: new Date().toISOString(),
            sessionId: ws.sessionId,
            data: finalAudioData,  // ‚úÖ Direct base64 string (NOT nested!)
            chunkIndex: sequenceNumber,
            totalChunks: index === sentences.length - 1 ? sentences.length : undefined
          };
          
          ws.send(JSON.stringify(ttsMsg));
          
        } catch (error) {
          console.error(`[STREAMING TTS] ‚ùå Failed chunk ${index + 1}: ${error}`);
          
          // Send error message for failed chunk
          const errorMsg: VoiceMessage = {
            type: 'ERROR',
            timestamp: new Date().toISOString(),
            code: 'TTS_GENERATION_FAILED',
            message: `Failed to generate TTS for chunk ${sequenceNumber}`,
            recoverable: true
          };
          ws.send(JSON.stringify(errorMsg));
        }
      });
      
      // Don't await all - let them stream as they complete!
      // But track completion
      Promise.all(ttsPromises).then(() => {
        // Send TTS end notification (matches TTSEndMessage)
        const endMsg: TTSEndMessage = {
          type: 'TTS_END',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          totalChunks: sentences.length
        };
        ws.send(JSON.stringify(endMsg));
        
        ws.isTTSActive = false;
        console.log(`[STREAMING TTS] ‚úÖ All ${sentences.length} chunks sent`);
      }).catch(error => {
        console.error('[STREAMING TTS] Error in parallel generation:', error);
      });
      
    } catch (error) {
      console.error('[STREAMING TTS] Setup error:', error);
      // Fallback to old method
      await this.streamTTSChunks(ws, text, language, emotion, intent, personaId);
    }
  }

  /**
   * Stream TTS chunks with emotion, intent, and persona support (AI Tutor pipeline)
   * Uses EnhancedVoiceService for emotion-based prosody and math-to-speech
   */
  async streamTTSChunks(
    ws: VoiceWebSocketClient,
    text: string,
    language: 'hi' | 'en',
    emotion?: string,
    intent?: string,
    personaId?: string
  ): Promise<void> {
    try {
      console.log(`[VOICE TTS] Converting with emotion: ${emotion}, intent: ${intent}, persona: ${personaId}`);
      
      // Use EnhancedVoiceService to apply emotion, intent, and persona
      const voiceOptions = {
        emotion,
        intent,
        personaId,
        language,
        enableMathSpeech: true,
        enablePauses: true,
        enableEmphasis: true
      };
      
      // Convert to speech with enhanced prosody
      const audioBuffer = await enhancedVoiceService.synthesize(text, voiceOptions);
      
      // Stream the enhanced audio chunks
      await this.streamTTSAudioDirect(ws, audioBuffer, language);
      
    } catch (error) {
      console.error('[VOICE TTS] Enhanced TTS error:', error);
      
      // Fallback to basic TTS without emotion/prosody
      await this.streamTTSAudio(ws, text, language);
    }
  }

  /**
   * Stream pre-generated audio buffer directly to client
   */
  private async streamTTSAudioDirect(
    ws: VoiceWebSocketClient,
    audioBuffer: Buffer,
    language: 'hi' | 'en'
  ): Promise<void> {
    try {
      console.log(`[VOICE STREAM] Starting direct audio streaming: ${audioBuffer.length} bytes`);
      
      // Mark TTS as active
      ws.isTTSActive = true;

      // Send TTS start notification
      const startMsg: TTSStartMessage = {
        type: 'TTS_START',
        timestamp: new Date().toISOString(),
        sessionId: ws.sessionId,
        text: '' // Already processed
      };
      ws.send(JSON.stringify(startMsg));

      // Split audio into chunks for streaming (10KB chunks)
      const CHUNK_SIZE = 10 * 1024; // 10KB
      const totalChunks = Math.ceil(audioBuffer.length / CHUNK_SIZE);
      
      console.log(`[VOICE STREAM] Streaming ${totalChunks} audio chunks`);

      for (let i = 0; i < totalChunks; i++) {
        // Check if interrupted
        if (!ws.isTTSActive) {
          console.log('[VOICE STREAM] TTS interrupted at chunk', i);
          break;
        }

        const start = i * CHUNK_SIZE;
        const end = Math.min(start + CHUNK_SIZE, audioBuffer.length);
        const chunk = audioBuffer.slice(start, end);

        const chunkMsg: TTSChunkMessage = {
          type: 'TTS_CHUNK',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          data: chunk.toString('base64'),
          chunkIndex: i,
          totalChunks: i === totalChunks - 1 ? totalChunks : undefined
        };

        ws.send(JSON.stringify(chunkMsg));

        // üöÄ OPTIMIZATION: Reduced delay for faster streaming (10ms instead of 50ms)
        await new Promise(resolve => setTimeout(resolve, 10));
      }

      // Send TTS end notification
      if (ws.isTTSActive) {
        const endMsg: TTSEndMessage = {
          type: 'TTS_END',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          totalChunks
        };
        ws.send(JSON.stringify(endMsg));
        
        console.log(`[VOICE STREAM] ‚úÖ Direct streaming complete: ${totalChunks} chunks sent`);
      }

      ws.isTTSActive = false;
    } catch (error) {
      console.error('[VOICE STREAM] Direct streaming error:', error);
      ws.isTTSActive = false;
      throw error;
    }
  }

  /**
   * Stream TTS audio chunks to client in real-time
   * Supports Sarvam Bulbul (primary) and AWS Polly (fallback)
   */
  async streamTTSAudio(
    ws: VoiceWebSocketClient,
    text: string,
    language: 'hi' | 'en',
    speaker?: string,
    pitch?: number,
    pace?: number,
    loudness?: number
  ): Promise<void> {
    try {
      console.log(`[VOICE STREAM] Starting TTS streaming for: "${text.substring(0, 50)}..."`);
      
      // Mark TTS as active
      ws.isTTSActive = true;

      // Send TTS start notification
      const startMsg: TTSStartMessage = {
        type: 'TTS_START',
        timestamp: new Date().toISOString(),
        sessionId: ws.sessionId,
        text: text.substring(0, 100)
      };
      ws.send(JSON.stringify(startMsg));

      // Generate TTS audio (Sarvam primary, Polly fallback)
      const audioBuffer = await this.synthesizeSpeech(text, language, speaker, pitch, pace, loudness);

      // Check if TTS was interrupted
      if (!ws.isTTSActive) {
        console.log('[VOICE STREAM] TTS was interrupted, aborting stream');
        return;
      }

      // Split audio into chunks for streaming (10KB chunks)
      const CHUNK_SIZE = 10 * 1024; // 10KB
      const totalChunks = Math.ceil(audioBuffer.length / CHUNK_SIZE);
      
      console.log(`[VOICE STREAM] Streaming ${totalChunks} audio chunks (${audioBuffer.length} bytes total)`);

      for (let i = 0; i < totalChunks; i++) {
        // Check if interrupted
        if (!ws.isTTSActive) {
          console.log('[VOICE STREAM] TTS interrupted at chunk', i);
          break;
        }

        const start = i * CHUNK_SIZE;
        const end = Math.min(start + CHUNK_SIZE, audioBuffer.length);
        const chunk = audioBuffer.slice(start, end);

        const chunkMsg: TTSChunkMessage = {
          type: 'TTS_CHUNK',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          data: chunk.toString('base64'),
          chunkIndex: i,
          totalChunks: i === totalChunks - 1 ? totalChunks : undefined
        };

        ws.send(JSON.stringify(chunkMsg));
        
        // Small delay between chunks for smoother streaming (adjust based on network)
        await new Promise(resolve => setTimeout(resolve, 50));
      }

      // Send TTS end notification
      if (ws.isTTSActive) {
        const endMsg: TTSEndMessage = {
          type: 'TTS_END',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          totalChunks
        };
        ws.send(JSON.stringify(endMsg));
        
        console.log(`[VOICE STREAM] ‚úÖ TTS streaming complete: ${totalChunks} chunks sent`);
      }

      ws.isTTSActive = false;
    } catch (error) {
      console.error('[VOICE STREAM] TTS streaming error:', error);
      ws.isTTSActive = false;
      
      const errorMsg: VoiceMessage = {
        type: 'ERROR',
        timestamp: new Date().toISOString(),
        code: 'TTS_STREAMING_ERROR',
        message: 'Failed to stream TTS audio',
        recoverable: true
      };
      ws.send(JSON.stringify(errorMsg));
    }
  }

  /**
   * Synthesize speech using TTS Router (Azure ‚Üí Sarvam ‚Üí Google ‚Üí Polly with circuit breaker)
   */
  private async synthesizeSpeech(
    text: string,
    language: 'hi' | 'en',
    speaker?: string,
    pitch?: number,
    pace?: number,
    loudness?: number
  ): Promise<Buffer> {
    // Use TTS Router which handles Azure as primary provider with circuit breaker protection
    try {
      console.log('[VOICE STREAM] Using TTS Router for synthesis...');
      
      const result = await ttsRouter.synthesize(
        text,
        {
          languageCode: language,
          speed: pace,
          pitch,
        },
        'avatar' // Use 'avatar' context for best quality (Azure primary)
      );
      
      console.log(`[VOICE STREAM] ‚úÖ TTS Router generated: ${result.audioBuffer.length} bytes (provider: ${result.provider})`);
      return result.audioBuffer;
    } catch (error) {
      console.error('[VOICE STREAM] TTS Router failed:', error);
      throw error;
    }
  }

  /**
   * Stop TTS streaming immediately
   */
  stopTTSStream(ws: VoiceWebSocketClient): void {
    if (ws.isTTSActive) {
      console.log(`[VOICE STREAM] Stopping TTS stream for session ${ws.sessionId}`);
      ws.isTTSActive = false;
    }
  }

  /**
   * Clear audio buffer
   */
  clearAudioBuffer(ws: VoiceWebSocketClient): void {
    if (ws.audioBuffer) {
      console.log(`[VOICE STREAM] Clearing audio buffer: ${ws.audioBuffer.length} chunks`);
      ws.audioBuffer = [];
    }
  }

  /**
   * üöÄ HELPER: Generate and stream TTS for a single sentence IMMEDIATELY
   * Used for TRUE real-time streaming during AI response generation
   * Now supports PHONEME_TTS_CHUNK for Unity lip-sync!
   */
  private async generateAndStreamSentenceTTS(
    ws: VoiceWebSocketClient,
    sentence: string,
    sequenceNumber: number,
    isLast: boolean,
    voiceOptions: {
      emotion?: string;
      intent?: string;
      personaId?: string;
      language: 'hi' | 'en';
      enableMathSpeech?: boolean;
      enablePauses?: boolean;
      enableEmphasis?: boolean;
      enablePhonemes?: boolean;  // üé§ NEW: Enable phoneme generation for lip-sync
    },
    ttsInFlightMap?: Map<string, Promise<void>>  // üî• ATOMIC: Track in-flight TTS promises
  ): Promise<void> {
    // üî• CRITICAL: Clean and normalize sentence for deduplication
    // Remove emojis, trailing punctuation, extra whitespace
    const cleanedSentence = sentence.trim()
      .replace(/[\u{1F600}-\u{1F64F}]|[\u{1F300}-\u{1F5FF}]|[\u{1F680}-\u{1F6FF}]|[\u{1F1E0}-\u{1F1FF}]|[\u{2600}-\u{26FF}]|[\u{2700}-\u{27BF}]/gu, '')
      .replace(/[‡•§.!?]+$/g, '')
      .replace(/\s+/g, ' ')
      .trim()
      .toLowerCase();

    // Skip empty sentences
    if (!cleanedSentence || cleanedSentence.length < 2) {
      console.log(`[TTS DEDUP] ‚ö†Ô∏è Skipping empty/too short sentence: "${sentence}"`);
      return;
    }

    // üî• ATOMIC DEDUP: Check if sentence is already being processed or was processed
    if (ttsInFlightMap) {
      if (ttsInFlightMap.has(cleanedSentence)) {
        console.log(`[TTS DEDUP] ‚ö†Ô∏è Skipping duplicate TTS (already in-flight or done): "${cleanedSentence.substring(0, 40)}..."`);
        return; // Exit immediately - sentence already processing/done!
      }

      // üî• CRITICAL FIX: Create promise AND add to map ATOMICALLY (single synchronous operation)
      // This prevents race conditions where two simultaneous calls both pass the has() check
      const ttsPromise = (async () => {
        try {
          await this.executeTTSGeneration(
            ws,
            sentence,
            sequenceNumber,
            isLast,
            voiceOptions
          );
        } catch (error) {
          console.error(`[TTS ERROR] Failed for sentence: "${cleanedSentence.substring(0, 40)}..."`, error);
          // Don't remove from map on error - prevents retry spam
          throw error;
        } finally {
          // üéØ CLEANUP: Remove from map after a delay to prevent memory leak
          // But keep it long enough to catch rapid duplicates (30 seconds)
          setTimeout(() => {
            ttsInFlightMap.delete(cleanedSentence);
          }, 30000);
        }
      })();

      // üéØ ATOMIC: Set in map IMMEDIATELY after creating promise (no await between check and set!)
      ttsInFlightMap.set(cleanedSentence, ttsPromise);

      // Don't await - fire and forget (dedup is already handled)
      return;
    }

    // Fallback: No map provided, execute directly (shouldn't happen)
    await this.executeTTSGeneration(ws, sentence, sequenceNumber, isLast, voiceOptions);
  }

  /**
   * üî• EXTRACTED: Actual TTS generation logic (separated from dedup logic)
   */
  private async executeTTSGeneration(
    ws: VoiceWebSocketClient,
    sentence: string,
    sequenceNumber: number,
    isLast: boolean,
    voiceOptions: {
      emotion?: string;
      intent?: string;
      personaId?: string;
      language: 'hi' | 'en';
      enableMathSpeech?: boolean;
      enablePauses?: boolean;
      enableEmphasis?: boolean;
      enablePhonemes?: boolean;
    }
  ): Promise<void> {
    // üî• CRITICAL: Check if avatar can still accept TTS
    if (ws.sessionId && !avatarStateService.canGenerateTTS(ws.sessionId)) {
      console.log(`[TTS GENERATION] ‚è≠Ô∏è Skipping - Avatar closed/not ready for session ${ws.sessionId}`);
      return;
    }

    try {

      const startTime = Date.now();

      // üßπ CLEAN TEXT: Remove emojis, special chars, normalize for natural TTS
      const cleanedSentence = TTSTextProcessor.processForTTSLite(sentence);

      if (sentence !== cleanedSentence) {
        console.log(`[TTS CLEAN] Original: "${sentence.substring(0, 50)}..."`);
        console.log(`[TTS CLEAN] Cleaned:  "${cleanedSentence.substring(0, 50)}..."`);
      }

      let audioBuffer: Buffer;
      let phonemes: Array<{time: number; blendshape: string; weight: number}> | undefined;
      let cached = false;

      // üé§ PHASE 1: Generate audio with or without phonemes
      if (voiceOptions.enablePhonemes) {
        // üé§ Generate audio + phonemes using multi-provider TTS router
        console.log(`[PHONEME STREAM] üé§ Generating audio + phonemes for sentence ${sequenceNumber}...`);

        const result = await ttsRouter.synthesizeWithPhonemes(
          cleanedSentence,
          { languageCode: voiceOptions.language === 'hi' ? 'hi-IN' : 'en-IN' },
          'avatar' // Use avatar context for best quality
        );

        audioBuffer = result.audioBuffer;

        // Map visemes/phonemes to Unity phonemes (if available)
        if (result.phonemes && result.phonemes.length > 0) {
          phonemes = mapPollyVisemesToUnityPhonemes(result.phonemes);
        }

        console.log(`[PHONEME STREAM] ‚úÖ Generated ${phonemes?.length || 0} phonemes for sentence ${sequenceNumber} (provider: ${result.provider})`);
      } else {
        // üöÄ Regular TTS without phonemes (check cache first)
        const cachedAudio = await ttsCacheService.get(
          cleanedSentence,
          voiceOptions.language,
          voiceOptions.emotion,
          voiceOptions.personaId
        );

        if (cachedAudio) {
          audioBuffer = cachedAudio;
          cached = true;
        } else {
          // Generate TTS audio using multi-provider router
          const result = await ttsRouter.synthesize(
            cleanedSentence,
            { languageCode: voiceOptions.language === 'hi' ? 'hi-IN' : 'en-IN' },
            'quick' // Use quick context for faster responses without phonemes
          );

          audioBuffer = result.audioBuffer;

          // Store in cache for future use
          await ttsCacheService.set(
            cleanedSentence,
            voiceOptions.language,
            audioBuffer,
            voiceOptions.emotion,
            voiceOptions.personaId
          );

          console.log(`[TRUE STREAM] TTS generated by provider: ${result.provider}, cached: ${result.cached}`);
        }
      }
      
      const genTime = Date.now() - startTime;
      const cacheStatus = voiceOptions.enablePhonemes ? 'üé§ WITH PHONEMES' : (cached ? 'üíæ CACHED' : 'üî® GENERATED');
      console.log(`[TRUE STREAM] ‚úÖ Sentence ${sequenceNumber} ${cacheStatus} (${genTime}ms): "${sentence.substring(0, 40)}..."`);
      
      // üöÄ PHASE 2: Send appropriate TTS chunk message
      const finalAudioData = audioBuffer.toString('base64');
      
      if (voiceOptions.enablePhonemes && phonemes) {
        // üé§ Send PHONEME_TTS_CHUNK with audio + phoneme data
        const phonemeMsg: PhonemeTTSChunkMessage = {
          type: 'PHONEME_TTS_CHUNK',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          audio: finalAudioData,
          phonemes: phonemes,
          chunkIndex: sequenceNumber,
          totalChunks: isLast ? sequenceNumber + 1 : undefined,
          text: sentence
        };
        
        ws.send(JSON.stringify(phonemeMsg));
      } else {
        // üîä Send regular TTS_CHUNK without phonemes
        const ttsMsg: TTSChunkMessage = {
          type: 'TTS_CHUNK',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          data: finalAudioData,  // ‚úÖ Direct base64 string (NOT nested!)
          chunkIndex: sequenceNumber,
          totalChunks: isLast ? sequenceNumber + 1 : undefined
        };
        
        ws.send(JSON.stringify(ttsMsg));
      }
      
      // üöÄ PHASE 3: Record metrics
      ttsMetrics.record({
        sentence,
        language: voiceOptions.language,
        generationTime: genTime,
        cached,
        compressed: false,
        audioSize: audioBuffer.length,
        sequence: sequenceNumber,
        sessionId: ws.sessionId,
      });
      
    } catch (error) {
      console.error(`[TRUE STREAM] ‚ùå Failed sentence ${sequenceNumber}: ${error}`);
      
      // Send error message (skip this chunk)
      const errorMsg: VoiceMessage = {
        type: 'ERROR',
        timestamp: new Date().toISOString(),
        code: 'TTS_GENERATION_FAILED',
        message: `Failed to generate TTS for chunk ${sequenceNumber}`,
        recoverable: true
      };
      ws.send(JSON.stringify(errorMsg));
    }
  }

  /**
   * Process transcribed text through complete AI Tutor pipeline and stream TTS response
   * Integrates 7-phase system, emotion detection, intent classification, dynamic prompts, and voice synthesis
   */
  async processTutorResponse(
    ws: VoiceWebSocketClient,
    transcribedText: string,
    chatId: string,
    userId: string,
    language: 'hi' | 'en'
  ): Promise<void> {
    try {
      console.log(`[VOICE TUTOR] Processing: "${transcribedText}" for chat ${chatId}`);

      // Get or create tutor session
      const chat = await storage.getChat(chatId);
      if (!chat) {
        throw new Error('Chat not found');
      }

      const user = await storage.getUser(userId);
      if (!user) {
        throw new Error('User not found');
      }

      const session = await tutorSessionService.getOrCreateSession(
        chatId,
        userId,
        chat.subject || 'General',
        chat.topic || 'General',
        user
      );

      // üî• STEP 1: LANGUAGE DETECTION with caching
      const startLangDetection = Date.now();
      const cachedLangResult = await performanceOptimizer.getCachedLanguageDetection(transcribedText);
      let langDetection = cachedLangResult;
      
      if (!cachedLangResult) {
        langDetection = await languageDetector.detectLanguage(transcribedText, {
          conversationHistory: [],
          userPreference: session.profileSnapshot?.preferredLanguage as DetectedLanguage,
          topic: session.topic
        });
        await performanceOptimizer.cacheLanguageDetection(transcribedText, langDetection);
      }
      
      const langDetectionTime = Date.now() - startLangDetection;
      const detectedLang = langDetection?.language || 'english';
      console.log(`[VOICE TUTOR] Language: ${detectedLang} (${langDetection?.confidence.toFixed(2)}) - ${langDetectionTime}ms`);
      
      // üî• STEP 2: SESSION CONTEXT - Add language detection
      await sessionContextManager.addLanguageDetection(
        userId,
        chatId,
        detectedLang,
        langDetection?.confidence || 0.5
      );

      // üî• STEP 3: INTENT CLASSIFICATION + EMOTION DETECTION (parallel)
      const [intentResult, emotionResult] = await Promise.all([
        intentClassifier.classify(transcribedText, {
          currentPhase: session.currentPhase,
          currentTopic: session.topic,
          isInPracticeMode: session.currentPhase === 'practice'
        }),
        emotionDetector.detectEmotion(transcribedText, [], language)
      ]);

      console.log(`[VOICE TUTOR] Intent: ${intentResult.intent} (${(intentResult.confidence * 100).toFixed(0)}%) | Emotion: ${emotionResult.emotion}`);
      
      // Add emotion to session context
      await sessionContextManager.addEmotionDetection(
        userId,
        chatId,
        emotionResult.emotion,
        emotionResult.confidence
      );
      
      const sessionCtx = await sessionContextManager.getContext(userId, chatId);

      // üî• STEP 4: HANDLE SPECIAL INTENTS (hints, phase advancement)
      if (intentResult.intent === 'request_hint') {
        const hintState = hintService.getHintState(await storage.getChatMessages(chatId, 50)) || 
                         hintService.initializeHintState();
        const advanceResult = hintService.advanceHintLevel(hintState);
        
        if (!advanceResult.canAdvance) {
          // Send hint limit message as TTS
          await this.streamTTSChunks(ws, advanceResult.message || 'No more hints available', language, emotionResult.emotion, intentResult.intent);
          return;
        }
        
        // Generate hint with AI (simplified for voice - no streaming)
        const hintPrompt = hintService.buildHintPrompt(
          advanceResult.nextLevel,
          language,
          session.topic || 'General',
          transcribedText,
          hintState.previousHints
        );
        
        // Generate hint response
        const hintResponse = await optimizedAI.generateResponse(transcribedText, hintPrompt, {
          language: detectedLang === 'hinglish' ? 'hindi' : 'english',
          useCache: true
        });
        
        // Save hint message with metadata
        await storage.addMessage({
          chatId,
          role: 'assistant',
          content: hintResponse.response,
          tool: null,
          metadata: {
            hintState: hintService.updateHintStateWithResponse(advanceResult.newState, hintResponse.response),
            hintLevel: advanceResult.nextLevel,
            model: hintResponse.model,
            cost: hintResponse.cost
          } as any
        });
        
        // Stream hint as TTS
        await this.streamTTSChunks(ws, hintResponse.response, language, emotionResult.emotion, intentResult.intent);
        return;
      }

      // üî• STEP 5: ASSESSMENT PHASE - Analyze response
      if (session.currentPhase === 'assessment') {
        const assessmentResult = tutorSessionService.analyzeResponse(transcribedText);
        await tutorSessionService.recordAssessment(chatId, assessmentResult);
        console.log(`[VOICE TUTOR] Assessment: Level ${assessmentResult.level}, Score ${assessmentResult.score}`);
      }

      // üî• STEP 6: GENERATE DYNAMIC PROMPT with all context
      const promptResult = dynamicPromptEngine.generateSystemPrompt({
        detectedLanguage: detectedLang,
        preferredLanguage: session.profileSnapshot?.preferredLanguage as DetectedLanguage,
        languageConfidence: langDetection?.confidence || 0.5,
        currentEmotion: emotionResult.emotion,
        emotionConfidence: emotionResult.confidence,
        emotionalStability: sessionCtx?.emotionalHistory && sessionCtx.emotionalHistory.length > 0 ? 
          (sessionCtx.emotionalHistory.filter(e => e.emotion === emotionResult.emotion).length / sessionCtx.emotionalHistory.length) : 0.5,
        subject: session.subject,
        topic: session.topic,
        level: session.level || 'beginner',
        currentPhase: session.currentPhase,
        intent: intentResult.intent,
        misconceptions: session.adaptiveMetrics?.misconceptions || [],
        strongConcepts: session.adaptiveMetrics?.strongConcepts || []
      });
      
      const systemPrompt = promptResult.systemPrompt;
      console.log(`[VOICE TUTOR] Dynamic prompt: ${systemPrompt.length} chars | Phase: ${session.currentPhase}`);

      // üî• STEP 7: SAVE USER MESSAGE with full metadata
      await storage.addMessage({
        chatId,
        role: 'user',
        content: transcribedText,
        tool: null,
        metadata: {
          intent: intentResult.intent,
          intentConfidence: intentResult.confidence,
          emotion: emotionResult.emotion,
          emotionConfidence: emotionResult.confidence,
          detectedLanguage: detectedLang,
          languageConfidence: langDetection?.confidence || 0,
          voiceInput: true
        } as any
      });

      // üî• STEP 8: TRUE STREAMING - Generate AI response AND TTS in parallel sentence-by-sentence!
      const startAIGeneration = Date.now();

      // Send TTS_START to reset client queue state
      const startMsg: TTSStartMessage = {
        type: 'TTS_START',
        timestamp: new Date().toISOString(),
        sessionId: ws.sessionId,
        text: 'Generating response...'
      };
      ws.send(JSON.stringify(startMsg));
      ws.isTTSActive = true;

      // Sentence accumulator and sequence tracking
      let currentSentence = '';
      let fullResponse = '';
      let sentenceIndex = 0;
      const sentenceBoundary = /[‡•§.!?]\s+|[‡•§.!?]$/;

      // üî• CRITICAL: Clear TTS in-flight map for new session to prevent stale duplicates
      if (!ws.ttsInFlightMap) {
        ws.ttsInFlightMap = new Map<string, Promise<void>>();
      } else {
        console.log(`[VOICE TUTOR] üßπ Clearing ${ws.ttsInFlightMap.size} stale TTS promises from previous session`);
        ws.ttsInFlightMap.clear();
      }
      
      // Voice options for TTS
      const voiceOptions = {
        emotion: emotionResult.emotion,
        intent: intentResult.intent,
        personaId: session.personaId,
        language,
        enableMathSpeech: true,
        enablePauses: true,
        enableEmphasis: true,
        enablePhonemes: true  // üé§ Enable phoneme generation for Unity lip-sync via WebSocket!
      };
      
      // üöÄ Stream AI response with REAL-TIME sentence-by-sentence TTS generation!
      const aiResult = await optimizedAI.generateStreamingResponse(
        transcribedText,
        systemPrompt,
        '', // context (empty for voice queries)
        async (chunk: string, meta?: any) => {
          // Handle completion event (save metadata)
          if (meta?.type === 'complete') {
            console.log(`[VOICE TUTOR] ‚úÖ AI streaming complete - Model: ${meta.model}, Cost: $${meta.cost?.toFixed(6) || 0}`);
            
            // Process final partial sentence if exists
            if (currentSentence.trim().length > 0) {
              // üé≠ Check avatar state before TTS generation
              const canGenerateTTS = avatarStateService.canGenerateTTS(ws.sessionId || '');
              
              if (canGenerateTTS) {
                // üöÄ OPTIMIZATION: Fire-and-forget for parallel TTS generation (dedup handled by ttsInFlightMap)
                this.generateAndStreamSentenceTTS(
                  ws,
                  currentSentence.trim(),
                  sentenceIndex,
                  true, // isLast
                  voiceOptions,
                  ws.ttsInFlightMap  // üî• Pass SHARED in-flight Map (atomic dedup)
                ).catch(err => console.error('[VOICE TUTOR] TTS final sentence error:', err));
              } else {
                // üìù Avatar not ready - Send text-only response
                const textMsg: VoiceMessage = {
                  type: 'AI_RESPONSE_TEXT',
                  timestamp: new Date().toISOString(),
                  sessionId: ws.sessionId,
                  text: currentSentence.trim(),
                  messageId: `${ws.sessionId}-${sentenceIndex}-final`
                };
                ws.send(JSON.stringify(textMsg));
                console.log(`[VOICE TUTOR] üìù Avatar not ready - Sent final text-only: "${currentSentence.trim().substring(0, 40)}..."`);
              }
            }
            
            // Send TTS_END
            const endMsg: TTSEndMessage = {
              type: 'TTS_END',
              timestamp: new Date().toISOString(),
              sessionId: ws.sessionId,
              totalChunks: sentenceIndex + 1
            };
            ws.send(JSON.stringify(endMsg));
            ws.isTTSActive = false;
            
            return;
          }
          
          // Accumulate text chunks
          currentSentence += chunk;
          fullResponse += chunk;
          
          // Check for sentence boundary
          const match = currentSentence.match(sentenceBoundary);
          if (match) {
            // Extract complete sentence(s)
            const parts = currentSentence.split(sentenceBoundary);
            
            // Process all complete sentences (all except last part which may be incomplete)
            for (let i = 0; i < parts.length - 1; i++) {
              const sentence = parts[i].trim();
              if (sentence) {
                // üé≠ Check avatar state before TTS generation
                const canGenerateTTS = avatarStateService.canGenerateTTS(ws.sessionId || '');
                
                if (canGenerateTTS) {
                  // üöÄ OPTIMIZATION: Fire-and-forget for parallel TTS generation (dedup handled by ttsInFlightMap)
                  this.generateAndStreamSentenceTTS(
                    ws,
                    sentence,
                    sentenceIndex,
                    false, // not last
                    voiceOptions,
                    ws.ttsInFlightMap  // üî• Pass SHARED in-flight Map (atomic dedup)
                  ).catch(err => console.error(`[VOICE TUTOR] TTS sentence ${sentenceIndex} error:`, err));
                } else {
                  // üìù Avatar not ready - Send text-only response
                  const textMsg: VoiceMessage = {
                    type: 'AI_RESPONSE_TEXT',
                    timestamp: new Date().toISOString(),
                    sessionId: ws.sessionId,
                    text: sentence,
                    messageId: `${ws.sessionId}-${sentenceIndex}`
                  };
                  ws.send(JSON.stringify(textMsg));
                  console.log(`[VOICE TUTOR] üìù Avatar not ready - Sent text-only: "${sentence.substring(0, 40)}..."`);
                }
                
                sentenceIndex++;
              }
            }
            
            // Keep the incomplete part for next iteration
            currentSentence = parts[parts.length - 1] || '';
          }
        }
      );
      
      const aiGenerationTime = Date.now() - startAIGeneration;
      console.log(`[VOICE TUTOR] ‚úÖ TRUE STREAMING complete: ${fullResponse.length} chars - ${aiGenerationTime}ms total`);

      // üî• STEP 9: VALIDATE RESPONSE QUALITY (after streaming)
      const startValidation = Date.now();
      const validation = await responseValidator.validate(fullResponse, {
        expectedLanguage: detectedLang,
        userEmotion: emotionResult.emotion,
        currentPhase: session.currentPhase,
        subject: session.subject || 'General',
        topic: session.topic || 'General',
        userMessage: transcribedText
      });
      const validationTime = Date.now() - startValidation;
      console.log(`[VOICE TUTOR] Validation: ${(validation.overallScore * 100).toFixed(1)}% - Valid: ${validation.isValid} (${validationTime}ms)`);

      // üéØ STEP 9.5: Generate proper SSML using dual output (for speaker button replay)
      console.log(`[VOICE TUTOR] üîÑ Generating proper SSML for voice response using dual output...`);
      
      const { generateDualOutput } = await import('./aiDualOutput');
      
      let chatMarkdown = fullResponse; // Default to streaming response
      let speakSSML = '';
      let speakMeta: any = {};
      let dualOutputSource = 'fallback';
      
      try {
        // Get recent context for dual output
        const contextMessages = await storage.getChatMessages(chatId, 5);
        
        // Map personaId to dual output persona (Garima ‚Üí Priya for female voice)
        const dualOutputPersona = session.personaId === 'garima' ? 'Priya' : 
                                  session.personaId === 'amit' ? 'Amit' : 'Priya';
        
        const dualOutput = await generateDualOutput({
          userQuery: transcribedText,
          contextMessages: contextMessages
            .filter(m => m.role !== 'assistant' || m.content !== fullResponse) // Exclude current response
            .map(m => ({
              role: m.role as 'user' | 'assistant',
              content: m.content
            })),
          persona: dualOutputPersona,
          language: language as 'en' | 'hi' | 'hinglish',
          emotion: emotionResult.emotion,
          subject: session.subject || 'General'
        });
        
        chatMarkdown = dualOutput.chat_md || fullResponse; // üìù Use rich markdown for display
        speakSSML = dualOutput.speak_ssml;
        speakMeta = dualOutput.speak_meta;
        dualOutputSource = dualOutput.metadata?.source || 'ai';
        
        console.log(`[VOICE TUTOR] ‚úÖ Dual output generated - chat_md: ${chatMarkdown.substring(0, 50)}... | speak_ssml: ${speakSSML.substring(0, 50)}...`);
      } catch (error) {
        console.error('[VOICE TUTOR] ‚ö†Ô∏è Dual output failed, using fallback SSML:', error);
        
        // Fallback: Basic SSML wrapping
        const { sanitizeSSML } = await import('../utils/ssmlUtils');
        const plainText = fullResponse
          .replace(/[*_#`]/g, '')
          .replace(/\[([^\]]+)\]\([^)]+\)/g, '$1')
          .trim();
        speakSSML = sanitizeSSML(`<speak>${plainText}</speak>`);
        speakMeta = {
          persona: session.personaId as 'Priya' | 'Amit',
          language: language as 'hi' | 'en' | 'hinglish',
          emotion: emotionResult.emotion
        };
      }

      // üî• STEP 10: SAVE AI RESPONSE with comprehensive metadata + SSML
      await storage.addMessage({
        chatId,
        role: 'assistant',
        content: chatMarkdown, // üìù Save rich markdown (chat_md) for display
        tool: null,
        metadata: {
          speakSSML,
          speakMeta,
          model: aiResult.model,
          cost: aiResult.cost,
          cached: aiResult.cached,
          personaId: session.personaId,
          emotion: emotionResult.emotion,
          phase: session.currentPhase,
          voiceOutput: true,
          streamingTTS: true,
          dualOutputSource,
          validation: {
            isValid: validation.isValid,
            overallScore: validation.overallScore,
            languageMatchScore: validation.layers.languageMatch.score,
            toneScore: validation.layers.toneAppropriate.score,
            qualityScore: validation.layers.educationalQuality.score,
            safetyScore: validation.layers.safety.score
          },
          timings: {
            languageDetection: langDetectionTime,
            aiGeneration: aiGenerationTime,
            validation: validationTime,
            total: langDetectionTime + aiGenerationTime + validationTime
          }
        } as any
      });

      console.log(`[VOICE TUTOR] ‚úÖ Complete pipeline finished for session ${ws.sessionId}`);

    } catch (error) {
      console.error('[VOICE TUTOR] Pipeline error:', error);
      
      const errorMsg = {
        type: 'ERROR',
        timestamp: new Date().toISOString(),
        sessionId: ws.sessionId,
        code: 'TUTOR_PIPELINE_ERROR',
        message: error instanceof Error ? error.message : 'AI Tutor pipeline failed',
        recoverable: true
      };
      
      ws.send(JSON.stringify(errorMsg));
    }
  }

  /**
   * Process text query through AI Tutor pipeline and stream response via WebSocket
   * PHASE 2: Unified WebSocket streaming for text chat
   */
  async processTextQuery(
    ws: VoiceWebSocketClient,
    queryText: string,
    chatId: string,
    language: 'hi' | 'en'
  ): Promise<void> {
    try {
      console.log(`[TEXT QUERY] Processing: "${queryText.substring(0, 50)}..." for chat ${chatId}`);

      if (!ws.userId) {
        throw new Error('User ID not found on WebSocket connection');
      }

      const userId = ws.userId;

      // Get chat and user
      const chat = await storage.getChat(chatId);
      if (!chat) {
        throw new Error('Chat not found');
      }

      const user = await storage.getUser(userId);
      if (!user) {
        throw new Error('User not found');
      }

      // Get or create tutor session
      const session = await tutorSessionService.getOrCreateSession(
        chatId,
        userId,
        chat.subject || 'General',
        chat.topic || 'General',
        user
      );

      // Language detection
      const langDetection = await languageDetector.detectLanguage(queryText, {
        conversationHistory: [],
        userPreference: session.profileSnapshot?.preferredLanguage as DetectedLanguage,
        topic: session.topic
      });
      const detectedLang = langDetection?.language || 'english';

      // Intent classification + Emotion detection (parallel)
      const [intentResult, emotionResult] = await Promise.all([
        intentClassifier.classify(queryText, {
          currentPhase: session.currentPhase,
          currentTopic: session.topic,
          isInPracticeMode: session.currentPhase === 'practice'
        }),
        emotionDetector.detectEmotion(queryText, [], language)
      ]);

      console.log(`[TEXT QUERY] Intent: ${intentResult.intent} | Emotion: ${emotionResult.emotion}`);

      // Generate dynamic prompt
      const promptResult = dynamicPromptEngine.generateSystemPrompt({
        detectedLanguage: detectedLang,
        preferredLanguage: session.profileSnapshot?.preferredLanguage as DetectedLanguage,
        languageConfidence: langDetection?.confidence || 0.5,
        currentEmotion: emotionResult.emotion,
        emotionConfidence: emotionResult.confidence,
        emotionalStability: 0.5,
        subject: session.subject,
        topic: session.topic,
        level: session.level || 'beginner',
        currentPhase: session.currentPhase,
        intent: intentResult.intent,
        misconceptions: session.adaptiveMetrics?.misconceptions || [],
        strongConcepts: session.adaptiveMetrics?.strongConcepts || []
      });

      const systemPrompt = promptResult.systemPrompt;

      // Save user message
      const userMessage = await storage.addMessage({
        chatId,
        role: 'user',
        content: queryText,
        tool: null,
        metadata: {
          intent: intentResult.intent,
          emotion: emotionResult.emotion,
          detectedLanguage: detectedLang,
          confidence: intentResult.confidence,
          source: 'text_websocket'
        } as any
      });

      // üîß FIX: Fetch conversation history AFTER saving user message
      const messages = await storage.getChatMessages(chatId);
      const conversationHistory = messages
        .slice(0, -1) // Exclude the last message (current user message)
        .map(m => `${m.role === 'user' ? 'Student' : 'Tutor'}: ${m.content}`)
        .join('\n\n');

      console.log(`[TEXT QUERY] üîç Context: ${messages.length} messages in history`);

      // Generate messageId for this response
      const messageId = `${ws.sessionId}-${Date.now()}`;

      // Stream AI response
      let fullResponse = '';
      let sentenceIndex = 0;
      let currentSentence = '';

      // üî• CRITICAL: Clear TTS in-flight map for new session to prevent stale duplicates
      if (!ws.ttsInFlightMap) {
        ws.ttsInFlightMap = new Map<string, Promise<void>>();
      } else {
        console.log(`[TEXT QUERY] üßπ Clearing ${ws.ttsInFlightMap.size} stale TTS promises from previous session`);
        ws.ttsInFlightMap.clear();
      }

      await optimizedAI.generateStreamingResponse(
        queryText,
        systemPrompt,
        conversationHistory,
        async (chunk, metadata) => {
          if (metadata.type === 'chunk' && chunk) {
            fullResponse += chunk;
            currentSentence += chunk;

            // Split into sentences for TTS
            const sentenceBoundary = /[‡•§.!?]\s+|[‡•§.!?]$/;
            const parts = currentSentence.split(sentenceBoundary);

            // Process complete sentences
            if (parts.length > 1) {
              for (let i = 0; i < parts.length - 1; i++) {
                const sentence = parts[i].trim();
                if (sentence) {
                  // Check if avatar is ready for TTS
                  const canGenerateTTS = avatarStateService.canGenerateTTS(ws.sessionId || '');

                  if (canGenerateTTS) {
                    // Generate TTS with phonemes
                    await this.generateAndStreamSentenceTTS(
                      ws,
                      sentence,
                      sentenceIndex,
                      false,
                      {
                        emotion: emotionResult.emotion,
                        intent: intentResult.intent,
                        personaId: session.personaId,
                        language,
                        enableMathSpeech: true,
                        enablePauses: true,
                        enableEmphasis: true,
                        enablePhonemes: true
                      },
                      ws.ttsInFlightMap  // üî• Pass SHARED in-flight Map (atomic dedup)
                    );
                  }

                  // Always send text chunk (for display in chat)
                  const chunkMsg: VoiceMessage = {
                    type: 'AI_RESPONSE_CHUNK',
                    timestamp: new Date().toISOString(),
                    sessionId: ws.sessionId,
                    content: sentence + ' ',
                    messageId,
                    isFirst: sentenceIndex === 0,
                    chunkIndex: sentenceIndex // üî¢ Add sequence number for deduplication
                  };
                  ws.send(JSON.stringify(chunkMsg));

                  sentenceIndex++;
                }
              }
              currentSentence = parts[parts.length - 1] || '';
            }
          }
        }
      );

      // Handle remaining text
      if (currentSentence.trim()) {
        const canGenerateTTS = avatarStateService.canGenerateTTS(ws.sessionId || '');
        if (canGenerateTTS) {
          await this.generateAndStreamSentenceTTS(
            ws,
            currentSentence.trim(),
            sentenceIndex,
            true,
            {
              emotion: emotionResult.emotion,
              intent: intentResult.intent,
              personaId: session.personaId,
              language,
              enableMathSpeech: true,
              enablePauses: true,
              enableEmphasis: true,
              enablePhonemes: true
            },
            ws.ttsInFlightMap  // üî• Pass SHARED in-flight Map (atomic dedup)
          );
        }

        const chunkMsg: VoiceMessage = {
          type: 'AI_RESPONSE_CHUNK',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          content: currentSentence.trim(),
          messageId,
          isFirst: sentenceIndex === 0,
          chunkIndex: sentenceIndex // üî¢ Add sequence number for deduplication
        };
        ws.send(JSON.stringify(chunkMsg));
      }

      // Send TTS_END if TTS was generated
      if (avatarStateService.canGenerateTTS(ws.sessionId || '')) {
        const endMsg: VoiceMessage = {
          type: 'TTS_END',
          timestamp: new Date().toISOString(),
          sessionId: ws.sessionId,
          totalChunks: sentenceIndex + 1
        };
        ws.send(JSON.stringify(endMsg));
      }

      // üéØ Generate proper SSML using dual output service (post-streaming)
      console.log(`[TEXT QUERY] üîÑ Generating proper SSML for final message using dual output...`);
      
      const { generateDualOutput } = await import('./aiDualOutput');
      
      let speakSSML = '';
      let speakMeta: any = {};
      let dualOutputSource = 'fallback';
      
      try {
        // Get recent context for dual output
        const contextMessages = await storage.getChatMessages(chatId, 5);
        
        // Map personaId to dual output persona (Garima ‚Üí Priya for female voice)
        const dualOutputPersona = session.personaId === 'garima' ? 'Priya' : 
                                  session.personaId === 'amit' ? 'Amit' : 'Priya';
        
        const dualOutput = await generateDualOutput({
          userQuery: queryText,
          contextMessages: contextMessages
            .filter(m => m.role !== 'assistant' || m.content !== fullResponse) // Exclude current response
            .map(m => ({
              role: m.role as 'user' | 'assistant',
              content: m.content
            })),
          persona: dualOutputPersona,
          language: language as 'en' | 'hi' | 'hinglish',
          emotion: emotionResult.emotion
        });
        
        speakSSML = dualOutput.speak_ssml;
        speakMeta = dualOutput.speak_meta;
        dualOutputSource = dualOutput.metadata?.source || 'ai';
        
        console.log(`[TEXT QUERY] ‚úÖ Dual output SSML generated: ${speakSSML.substring(0, 50)}...`);
      } catch (error) {
        console.error('[TEXT QUERY] ‚ö†Ô∏è Dual output failed, using fallback SSML:', error);
        
        // Fallback: Basic SSML wrapping
        const { sanitizeSSML } = await import('../utils/ssmlUtils');
        const plainText = fullResponse
          .replace(/[*_#`]/g, '')
          .replace(/\[([^\]]+)\]\([^)]+\)/g, '$1')
          .trim();
        speakSSML = sanitizeSSML(`<speak>${plainText}</speak>`);
        speakMeta = {
          persona: session.personaId as 'Priya' | 'Amit',
          language: language as 'hi' | 'en' | 'hinglish',
          emotion: emotionResult.emotion
        };
      }

      // Send completion message
      const completeMsg: VoiceMessage = {
        type: 'AI_RESPONSE_COMPLETE',
        timestamp: new Date().toISOString(),
        sessionId: ws.sessionId,
        messageId,
        emotion: emotionResult.emotion,
        personaId: session.personaId,
        phase: session.currentPhase as any,
        phaseStep: session.phaseStep || 0,
        language
      };
      ws.send(JSON.stringify(completeMsg));

      // Save AI response WITH proper SSML metadata
      await storage.addMessage({
        chatId,
        role: 'assistant',
        content: fullResponse,
        tool: null,
        metadata: {
          speakSSML,
          speakMeta,
          emotion: emotionResult.emotion,
          personaId: session.personaId,
          phase: session.currentPhase,
          source: `text_websocket_${dualOutputSource}`,
          avatarTTS: avatarStateService.canGenerateTTS(ws.sessionId || '')
        } as any
      });

      console.log(`[TEXT QUERY] ‚úÖ Complete: ${fullResponse.length} chars streamed`);

    } catch (error) {
      console.error('[TEXT QUERY] Error:', error);

      const errorMsg: VoiceMessage = {
        type: 'ERROR',
        timestamp: new Date().toISOString(),
        sessionId: ws.sessionId,
        code: 'TEXT_QUERY_ERROR',
        message: error instanceof Error ? error.message : 'Text query processing failed',
        recoverable: true
      };

      ws.send(JSON.stringify(errorMsg));
    }
  }
}

// Export singleton instance
export const voiceStreamService = new VoiceStreamService();
